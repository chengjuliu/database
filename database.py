import os, re, time, io, logging, warnings
import pandas as pd
from PIL import Image
import torch
import torchaudio
import pdfplumber
import spacy
from openai import OpenAI
from transformers import (
    CLIPProcessor, CLIPModel,
    WhisperProcessor, WhisperForConditionalGeneration
)
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

# ğŸ“¦ æ—¥å¿—å’Œè­¦å‘ŠæŠ‘åˆ¶
logging.getLogger("pdfminer").setLevel(logging.ERROR)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', message='.*CropBox.*')
warnings.filterwarnings('ignore', message='.*MediaBox.*')

# âœ… spaCy ä¸­æ–‡æ¨¡å‹
nlp = spacy.load("zh_core_web_sm")

# âœ… DeepSeek é…ç½®
key = 'sk-6b461ad45209484b937b94b0693e6aa1'
api_url = "https://api.deepseek.com/v1"


def printChar(text, delay=0.05):
    for char in text:
        print(char, end='', flush=True)
        time.sleep(delay)
    print()


def sendToDeepSeek(say):
    client = OpenAI(api_key=key, base_url=api_url)
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œè¯·ç”¨æ­£å¼çš„è¯­æ°”å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"},
            {"role": "user", "content": say},
        ],
        stream=False
    )
    return response.choices[0].message.content


# âœ… å‘é‡æ¨¡å‹
embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


# âœ… PDF æŒ‰å¥æ‹†åˆ†
def load_pdf_chunks(pdf_path):
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    all_sentences = []
    sentence_id = 0
    for idx, doc in enumerate(docs):
        content = doc.page_content.replace("\n", "")
        content = re.sub(r'([ã€‚ï¼ï¼Ÿ.!?])', r"\1##SPLIT##", content)
        sentences = [s.strip() for s in content.split("##SPLIT##") if s.strip()]
        page_num = doc.metadata.get("page", idx + 1)
        for i, sentence in enumerate(sentences):
            sentence_id += 1
            meta = {
                "source": f"page_{page_num}",
                "page": page_num,
                "sentence_number": i + 1,
                "global_id": sentence_id
            }
            all_sentences.append(Document(page_content=sentence, metadata=meta))
    return all_sentences


# âœ… å›¾åƒåµŒå…¥
def embed_images(image_dir):
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    embeddings, image_count = [], 0
    for filename in os.listdir(image_dir):
        if filename.lower().endswith((".jpg", ".jpeg", ".png")):
            image_count += 1
            image = Image.open(os.path.join(image_dir, filename)).convert("RGB")
            inputs = processor(images=image, return_tensors="pt")
            outputs = model.get_image_features(**inputs)
            # ä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹é‡æ–°å¤„ç†å›¾åƒæè¿°
            image_text = f"å›¾åƒ_{image_count}"
            image_embedding = embedder.embed_query(image_text)
            embeddings.append((image_text, image_embedding))
    return embeddings, image_count


# âœ… éŸ³é¢‘è½¬å†™
def embed_audio(audio_dir):
    processor = WhisperProcessor.from_pretrained("openai/whisper-base")
    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-base")
    result = []
    for file in os.listdir(audio_dir):
        if file.endswith(".mp3") or file.endswith(".wav"):
            waveform, sr = torchaudio.load(os.path.join(audio_dir, file))
            inputs = processor(waveform.squeeze(), sampling_rate=sr, return_tensors="pt").input_features
            ids = model.generate(inputs)
            text = processor.batch_decode(ids, skip_special_tokens=True)[0]
            result.append((f"éŸ³é¢‘è½¬å½•ï¼š{text}", text))
    return result


# âœ… è¡¨æ ¼æå–
def extract_tables_from_pdf(pdf_path):
    tables = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, 1):
            page_tables = page.extract_tables()
            for table in page_tables:
                if table and len(table) > 1:
                    header = [cell or "" for cell in table[0]]
                    body = [[cell or "" for cell in row] for row in table[1:] if any(row)]
                    if header and body:
                        total = len(header) * len(body)
                        filled = sum(1 for row in body for c in row if c.strip())
                        tables.append({
                            "page": page_num,
                            "table_num": len(tables) + 1,
                            "header": header,
                            "content": body,
                            "quality_score": {
                                "completeness": round(filled / total, 2) if total else 0,
                                "cell_count": total
                            }
                        })
    return tables


# âœ… å¢å¼ºå…³ç³»æŠ½å–
def extract_information(docs):
    extracted = {"æ–‡æœ¬": [], "å…¬å¼": [], "å…³ç³»": [], "å›¾åƒ": [], "è¡¨æ ¼": []}
    for doc in docs:
        text = doc.page_content
        if not isinstance(text, str):
            continue
        extracted["æ–‡æœ¬"].append(text)

        # å…¬å¼æå–
        formulas = re.findall(r'[\w\d\(\)\+\-\*/=\^âˆšâˆ‘Ï€\[\]\{\}ï¼]+', text)
        formulas = [f for f in formulas if any(op in f for op in ['=', '^', 'âˆš', 'âˆ‘', 'Ï€']) and len(f) > 5]
        extracted["å…¬å¼"].extend(formulas)

        # ä½¿ç”¨ spaCy è¿›è¡Œå…³ç³»æŠ½å–
        parsed = nlp(text)
        for sent in parsed.sents:
            subj = [t for t in sent if 'subj' in t.dep_]
            obj = [t for t in sent if 'obj' in t.dep_ or t.dep_ == 'attr']
            root = [t for t in sent if t.dep_ == 'ROOT']
            ents = list(sent.ents)
            if subj and root and obj:
                extracted["å…³ç³»"].append((subj[0].text, root[0].text, obj[0].text))
            elif len(ents) >= 2 and root:
                extracted["å…³ç³»"].append((ents[0].text, root[0].text, ents[1].text))
    return extracted


# âœ… æ„å»ºå‘é‡åº“
def build_multimodal_vectorstore(pdf_path, image_dir, audio_dir, db_path):
    print("ğŸ“˜ åŠ è½½ PDF...")
    docs = load_pdf_chunks(pdf_path)
    print("ğŸ“Š æå–è¡¨æ ¼...")
    tables = extract_tables_from_pdf(pdf_path)
    print("ğŸ–¼ï¸ å›¾åƒåµŒå…¥...")
    image_embeds, img_count = embed_images(image_dir)
    print("ğŸ”Š éŸ³é¢‘è½¬å†™...")
    audio_embeds = embed_audio(audio_dir)

    # å°†éŸ³é¢‘è½¬å†™ç»“æœåŠ å…¥æ–‡æ¡£
    for text, _ in audio_embeds:
        docs.append(Document(page_content=text, metadata={"source": "audio"}))
    # å°†è¡¨æ ¼ä¿¡æ¯åŠ å…¥æ–‡æ¡£
    for table in tables:
        rows = "\n".join(str(dict(zip(table['header'], r))) for r in table['content'][:3])
        docs.append(Document(
            page_content=f"è¡¨æ ¼-ç¬¬{table['page']}é¡µ:\nè¡¨å¤´:{table['header']}\n{rows}",
            metadata={"source": f"table_page_{table['page']}", "type": "table"}
        ))

    clean_docs = [Document(page_content=d.page_content.strip(), metadata=d.metadata)
                  for d in docs if d.page_content.strip()]
    print(f"âœ… æ–‡æœ¬æ¡æ•°ï¼š{len(clean_docs)}")

    # ç”¨ PDF æ–‡æœ¬æ„å»ºåŸºç¡€å‘é‡åº“
    vs = FAISS.from_documents(clean_docs, embedder, normalize_L2=True)

    # å¤„ç†å›¾åƒåµŒå…¥ï¼šä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰å›¾åƒçš„æ–‡æœ¬å’Œå‘é‡
    if image_embeds:
        # å°†æ–‡æœ¬å’Œå‘é‡ç»„åˆæˆå…ƒç»„åˆ—è¡¨
        text_embeddings = [(text, vec) for text, vec in image_embeds]
        # å¦‚æœåªæœ‰ä¸€æ¡ï¼Œä¸ºé¿å…å†…éƒ¨ zip è§£åŒ…æ—¶å°†å­—ç¬¦ä¸²æ‹†åˆ†æˆå­—ç¬¦ï¼Œå¤åˆ¶ä¸€ä»½
        if len(text_embeddings) == 1:
            text_embeddings = text_embeddings * 2
        vs.add_embeddings(text_embeddings)
    vs.save_local(db_path)

    info = extract_information(clean_docs)
    info["å›¾åƒ"] = [f"å›¾åƒ_{i + 1}" for i in range(img_count)]
    info["è¡¨æ ¼"] = [f"{t['page']}é¡µ_è¡¨{t['table_num']}" for t in tables]

    print("\nğŸ“Š ä¿¡æ¯æŠ½å–ç»Ÿè®¡ï¼š")
    df = pd.DataFrame({
        "ç±»å‹": list(info.keys()),
        "æ•°é‡": [len(v) for v in info.values()],
        "æ ·ä¾‹": [str(v[:2]) for v in info.values()]
    })
    print(df.to_string(index=False))


# âœ… æŸ¥è¯¢å‘é‡åº“
def query_vectorstore(query, db_path):
    vs = FAISS.load_local(db_path, embedder, allow_dangerous_deserialization=True)
    results = vs.similarity_search(query, k=3)
    return "\n".join([f"{d.metadata.get('source', '')} ç¬¬{d.metadata.get('sentence_number', '?')}å¥ï¼š{d.page_content}"
                      for d in results])


# âœ… ä¸»ç¨‹åº
if __name__ == "__main__":
    pdf_path = r"C:\Users\lcj\Desktop\test.pdf"
    image_dir = "./images"
    audio_dir = "./audios"
    db_path = "./mm_db"

    if not os.path.exists(db_path):
        build_multimodal_vectorstore(pdf_path, image_dir, audio_dir, db_path)

    while True:
        myin = input("æ‚¨è¯·è¯´ï¼š")
        if myin.lower() == 'bye':
            print("æ¬¢è¿ä¸‹æ¬¡ä½¿ç”¨ï¼å†è§ï¼")
            break
        related = query_vectorstore(myin, db_path)
        prompt = f"è¯·æ ¹æ®ä»¥ä¸‹èµ„æ–™å›ç­”é—®é¢˜ï¼š\n{related}\n\né—®é¢˜ï¼š{myin}"
        answer = sendToDeepSeek(prompt)
        printChar(answer)
        print("-----------------------------------------------------------")